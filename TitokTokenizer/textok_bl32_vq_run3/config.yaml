experiment:
  project: textok_bl32_vq
  name: textok_bl32_vq_run3
  output_dir: textok_bl32_vq_run3
  max_train_examples: 685800000
  save_every: 50000
  eval_every: 50000
  generate_every: 500
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  logging_dir: textok_bl32_vq_run3/logs
model:
  vq_model:
    quantize_mode: vq
    codebook_size: 8192
    token_size: 64
    use_l2_norm: false
    commitment_cost: 0.25
    clustering_vq: true
    vit_enc_model_size: base
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    is_legacy: false
    text_embed_dim: 768
losses:
  discriminator_start: 200000
  quantizer_weight: 1.0
  discriminator_factor: 1.0
  discriminator_weight: 0.1
  perceptual_loss: lpips-convnext_s-1.0-0.1
  perceptual_weight: 1.1
  reconstruction_loss: l2
  reconstruction_weight: 1.0
  lecam_regularization_weight: 0.001
dataset:
  params:
    train_shards_path_or_url: /data/san/HF_datasets/imgnet-1k-caption-wds/imagenet1k-train-{000000..000160}.tar
    eval_shards_path_or_url: /data/san/HF_datasets/imgnet-1k-caption-wds/imagenet1k-val-{000000..000049}.tar
    num_workers_per_gpu: 12
    dataset_with_class_label: false
    dataset_with_text_label: true
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
    res_ratio_filtering: true
    normalize_mean:
    - 0.485
    - 0.456
    - 0.406
    normalize_std:
    - 0.229
    - 0.224
    - 0.225
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    discriminator_learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 16
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 650000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/TexTok/textok_bl32_vq.yaml
